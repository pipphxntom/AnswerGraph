"""
Answer composition module using Qwen2-7B-Instruct LLM.

This module provides functions to generate structured answers from evidence
using the Qwen2-7B-Instruct model via transformers pipeline.
"""
import re
import logging
import asyncio
from typing import Dict, Any, List, Optional, Tuple
from functools import lru_cache
import threading

# Import transformers components
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# Import guards for quality checks
from src.rag.guards import numeric_consistency, require_citation

logger = logging.getLogger(__name__)

# Global variables for LLM components
MODEL_NAME = "Qwen/Qwen2-7B-Instruct"
_tokenizer = None
_model = None
_generation_pipeline = None
_model_lock = threading.Lock()


@lru_cache(maxsize=1)
def get_llm_pipeline():
    """
    Get or initialize the LLM pipeline.
    
    Uses lazy loading and caching to avoid loading the model multiple times.
    
    Returns:
        Transformers pipeline for text generation
    """
    global _tokenizer, _model, _generation_pipeline
    
    with _model_lock:
        if _generation_pipeline is None:
            logger.info(f"Initializing LLM pipeline with {MODEL_NAME}...")
            
            # Load tokenizer and model
            _tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
            _model = AutoModelForCausalLM.from_pretrained(
                MODEL_NAME,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # Create pipeline
            _generation_pipeline = pipeline(
                "text-generation",
                model=_model,
                tokenizer=_tokenizer,
                max_new_tokens=512,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                trust_remote_code=True
            )
            
            logger.info("LLM pipeline initialized successfully")
        
        return _generation_pipeline


def create_prompt(query: str, evidence: List[Dict[str, Any]]) -> str:
    """
    Create a prompt for the LLM based on the query and evidence.
    
    Args:
        query: User's query
        evidence: List of evidence documents
        
    Returns:
        Formatted prompt for the LLM
    """
    # Extract content from evidence
    evidence_texts = []
    for i, doc in enumerate(evidence):
        content = doc.get("content", doc.get("text", ""))
        if content:
            # Truncate long content
            if len(content) > 1000:
                content = content[:1000] + "..."
            
            # Add metadata
            source_info = f"[Source {i+1}"
            if doc.get("policy_id"):
                source_info += f", Policy: {doc['policy_id']}"
            if doc.get("page") or doc.get("page_number"):
                page = doc.get("page", doc.get("page_number"))
                source_info += f", Page: {page}"
            source_info += "]"
            
            evidence_texts.append(f"{source_info}\n{content}")
    
    # Combine evidence
    evidence_text = "\n\n".join(evidence_texts)
    
    # Create the prompt
    prompt = f"""You are an AI assistant that answers questions about policies and procedures based on provided evidence. 
Your answers must be accurate, concise, and based only on the evidence provided. 
Do not make up information or include details not in the evidence.

Follow this exact format for your response:
1. First line: Direct answer to the question
2. Then bullet points (•) for key details
3. End with "Source:" followed by the URL and page number of the primary source

EVIDENCE:
{evidence_text}

USER QUESTION:
{query}

YOUR ANSWER (following the exact format):
"""
    
    return prompt


def extract_structured_answer(generated_text: str) -> Dict[str, Any]:
    """
    Extract and structure the generated answer to follow the template.
    
    Args:
        generated_text: Raw text generated by the LLM
        
    Returns:
        Structured answer with direct_answer, key_points, and source
    """
    # Extract the answer part (after the prompt)
    answer_match = re.search(r"YOUR ANSWER[^:]*:(.+)$", generated_text, re.DOTALL)
    if answer_match:
        answer_text = answer_match.group(1).strip()
    else:
        # If marker not found, use the last part of the text
        answer_text = generated_text.split("\n\n")[-1].strip()
    
    # Extract the components
    lines = answer_text.split("\n")
    
    # Direct answer is the first non-empty line
    direct_answer = ""
    key_points = []
    source_text = ""
    
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        if not line:
            i += 1
            continue
        
        if not direct_answer:
            direct_answer = line
            i += 1
        elif line.startswith("•") or line.startswith("-") or re.match(r"^\d+\.", line):
            # This is a bullet point
            key_points.append(line.lstrip("•-. ").strip())
            i += 1
        elif line.lower().startswith("source:"):
            # This is the source line
            source_text = line
            # Combine with any additional source lines
            i += 1
            while i < len(lines) and not lines[i].strip().startswith("•") and not lines[i].strip().startswith("-"):
                source_text += " " + lines[i].strip()
                i += 1
        else:
            # If not a recognized format, treat as continuation of direct answer
            direct_answer += " " + line
            i += 1
    
    # Extract URL and page from source text
    url_match = re.search(r'https?://\S+', source_text)
    url = url_match.group(0) if url_match else None
    
    page_match = re.search(r'page\s*:?\s*(\d+)', source_text, re.IGNORECASE)
    page = int(page_match.group(1)) if page_match else None
    
    # Create structured answer
    structured_answer = {
        "direct_answer": direct_answer,
        "key_points": key_points,
        "source_text": source_text,
        "source": {
            "url": url,
            "page": page
        }
    }
    
    return structured_answer


def format_final_answer(structured_answer: Dict[str, Any]) -> str:
    """
    Format the structured answer into the final template.
    
    Args:
        structured_answer: Structured answer components
        
    Returns:
        Formatted answer text
    """
    # Start with the direct answer
    formatted_text = structured_answer["direct_answer"] + "\n\n"
    
    # Add bullet points
    for point in structured_answer["key_points"]:
        formatted_text += f"• {point}\n"
    
    # Add source
    formatted_text += "\n"
    if structured_answer["source"]["url"] and structured_answer["source"]["page"]:
        formatted_text += f"Source: {structured_answer['source']['url']} (Page {structured_answer['source']['page']})"
    elif structured_answer["source"]["url"]:
        formatted_text += f"Source: {structured_answer['source']['url']}"
    elif structured_answer["source_text"]:
        formatted_text += structured_answer["source_text"]
    else:
        formatted_text += "Source: Information from internal policy documents"
    
    return formatted_text


def extract_first_source_info(evidence: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Extract the URL and page from the first evidence item.
    
    Args:
        evidence: List of evidence documents
        
    Returns:
        Source information with URL and page
    """
    if not evidence:
        return {"url": None, "page": None}
    
    first_doc = evidence[0]
    
    # Extract URL
    url = None
    if "url" in first_doc:
        url = first_doc["url"]
    elif "source_url" in first_doc:
        url = first_doc["source_url"]
    
    # Extract page
    page = None
    if "page" in first_doc:
        page = first_doc["page"]
    elif "page_number" in first_doc:
        page = first_doc["page_number"]
    
    return {"url": url, "page": page}


async def compose_answer(
    query: str, 
    evidence: List[Dict[str, Any]]
) -> Optional[Dict[str, Any]]:
    """
    Compose an answer to a query based on evidence using an LLM.
    
    The answer follows a specific template:
    1. First line: Direct answer to the question
    2. Bullet points for key details
    3. Source information with URL and page
    
    The function enforces quality checks using numeric_consistency and require_citation.
    
    Args:
        query: User's query
        evidence: List of evidence documents
        
    Returns:
        Formatted answer or None if quality checks fail
    """
    if not evidence:
        logger.warning("No evidence provided for compose_answer")
        return None
    
    try:
        # Get evidence texts for consistency check
        evidence_texts = [doc.get("content", doc.get("text", "")) for doc in evidence]
        evidence_texts = [text for text in evidence_texts if text]
        
        # Extract first source info to use as fallback
        first_source = extract_first_source_info(evidence)
        
        # Create the prompt
        prompt = create_prompt(query, evidence)
        
        # Get or initialize the pipeline
        generator = get_llm_pipeline()
        
        # Generate text
        logger.info(f"Generating answer for query: {query}")
        response = generator(prompt, return_full_text=True)
        generated_text = response[0]["generated_text"]
        
        # Extract and structure the answer
        structured_answer = extract_structured_answer(generated_text)
        
        # If source URL or page is missing, use the first evidence source
        if not structured_answer["source"]["url"]:
            structured_answer["source"]["url"] = first_source["url"]
        if not structured_answer["source"]["page"]:
            structured_answer["source"]["page"] = first_source["page"]
        
        # Format the answer according to the template
        formatted_text = format_final_answer(structured_answer)
        
        # Check numeric consistency
        consistency_passed, _, missing_values = numeric_consistency(
            formatted_text, evidence_texts
        )
        
        if not consistency_passed:
            logger.warning(f"Numeric consistency check failed: {missing_values}")
            return None
        
        # Check citation
        answer_with_source = {
            "text": formatted_text,
            "sources": [
                {
                    "url": structured_answer["source"]["url"],
                    "page": structured_answer["source"]["page"]
                }
            ]
        }
        
        citation_passed, msg = require_citation(answer_with_source)
        
        if not citation_passed:
            logger.warning(f"Citation check failed: {msg}")
            return None
        
        # Prepare final result
        result = {
            "text": formatted_text,
            "direct_answer": structured_answer["direct_answer"],
            "key_points": structured_answer["key_points"],
            "sources": [
                {
                    "url": structured_answer["source"]["url"],
                    "page": structured_answer["source"]["page"],
                    "policy_id": evidence[0].get("policy_id") if evidence else None
                }
            ]
        }
        
        return result
    
    except Exception as e:
        logger.error(f"Error in compose_answer: {str(e)}")
        return None
